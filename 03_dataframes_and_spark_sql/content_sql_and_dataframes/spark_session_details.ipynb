{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"uva_seal.png\">  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark SQL and DataFrames\n",
    "\n",
    "### University of Virginia\n",
    "### DS 7200: Distributed Computing\n",
    "### Last Updated: August 20, 2023\n",
    "\n",
    "---  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SparkSession\n",
    "\n",
    "This notebook provides details about setting the SparkSession.\n",
    "\n",
    "The `SparkSession` is a unified conduit to all Spark operations and data.  It's an example of a `context manager`.  \n",
    "\n",
    "Here is an example of building a SparkSession with several configs:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---  \n",
    "```\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"local[*]\") \\                         # use all cores on local machine\n",
    "    .appName(\"Python Spark SQL basic example\") \\  # will see appName on cluster manager\n",
    "    .config(\"spark.executor.memory\", '20g') \\     # RAM per executor (worker)\n",
    "    .config('spark.executor.cores', '5') \\        # cores available to EACH executor\n",
    "    .config('spark.executor.instances', '17') \\   # total number of executors\n",
    "    .config(\"spark.driver.memory\",'1g') \\         # RAM for driver, generally lower need than a worker\n",
    "    .getOrCreate()\n",
    "    \n",
    "# for details see:\n",
    "# https://spark.apache.org/docs/latest/configuration.html\n",
    "```\n",
    "---  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up Cores, Executors, RAM \n",
    " \n",
    "- setting these configs is best codified in a function\n",
    "- Spark sets configs by default, but unfortunately they're not always optimal\n",
    "\n",
    "---  \n",
    "\n",
    "<span style=\"color:red\">**Example: Hardware consists of 6 nodes, each with 16 cores, 64GB RAM**</span>\n",
    "\n",
    "RESOURCE OVERHEAD:  \n",
    "$O1$. On each executor, 1 core and 1 GB RAM is consumed by OS and Hadoop Daemons  \n",
    "This leaves 15 available cores on each node  \n",
    "$O2$. The resource manager (e.g., YARN) will require an overhead ~1GB RAM per node  \n",
    "$O3$. One executor is required for the driver\n",
    "\n",
    "**Number of cores**  \n",
    "More cores means more concurrent processing, but an application running > 5 concurrent tasks generally doesn't perform well.  \n",
    "cap this at **spark.executor.cores = 5**.  \n",
    "\n",
    "**Executor instances**  \n",
    "We can set 15 cores_per_node / 5 cores_per_executor = 3 executors_per_node. 15 is due to $O1$.\n",
    "\n",
    "Given 6 nodes and 3 executors per node, we can set 18 executors  \n",
    "One of these executors is required for the driver $(O3)$    \n",
    "Thus, we set **spark.executor.instances = 17**  \n",
    "\n",
    "**Executor memory**  \n",
    "Available RAM is 63GB per node $(O1)$. For 3 executors per node, this gives 63GB/3 = 21GB per executor  \n",
    "The resource manager will require an overhead ~1GB per node $(O2)$. set **spark.executor.memory = 20g**\n",
    "\n",
    "**NOTE:** `spark.executor.cores` will use all cores by default (this is a simpler way to go, but not always optimal)  \n",
    "\n",
    "---  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DS7200 Spark 3.3",
   "language": "python",
   "name": "ds5110_spark3.3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
